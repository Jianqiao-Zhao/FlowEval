In each folder contains all the scores obtained either by an evaluation metric or by human evaluation. `reference_score.npy` means the results of human evaluation. The loading methods are written in the `calculate_correlation.py`.

Note that on Controllable Dialogue, we split the full dataset into a small valid set and a large test set. We use the valid set to tune hyperparameters of FlowEval and report the results on test set. Since other methods do not need hyperparameter tuning in general, the whole Controllable Dialogue can be used as the test set and we calculate the scores for every dialogue. For FlowEval, to keep the length of the resulting score list, we also calculate FlowEval on the whole dataset. Please remember to exclude those scores that comes from the valid set during correlation calculation.